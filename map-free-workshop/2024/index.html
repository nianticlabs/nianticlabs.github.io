<!doctype html>
<html lang="en">

<head>
    <title>Map-free Visual Relocalization: Metric Pose Relative to a Single Image</title>
    <meta name="description" content="Map-free visual relocalization ECCV 2024 workshop"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>
    <meta charset="utf-8"/>
    <link rel="icon" href="assets/images/NianticLogo.png"/>

    <!--Facebook preview-->
    <meta property="og:title"
          content="Map-free Visual Relocalization: Metric Pose Relative to a Single Image"/>
    <meta property="og:description"
          content="Map-free Visual Relocalization Competition. An ECCV'24 workshop."/>
    <meta property="og:url" content="https://TODO"/>

    <!--Twitter preview-->
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:title"
          content="Map-free Visual Relocalization: Metric Pose Relative to a Single Image"/>
    <meta name="twitter:description"
          content="Map-free Visual Relocalization Competition. An ECCV'24 workshop."/>

    <!--Style-->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH"
          crossorigin="anonymous"/>

    <link href="css/style.css" rel="stylesheet"
          onerror="this.onerror=null;this.href='https://storage.cloud.google.com/res-recon-experiments/aron-workshop24/20240702/style.css';"/>
    <script src="https://kit.fontawesome.com/746ee6bfa4.js" crossorigin="anonymous"></script>

    <!--<script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>-->
    <!--<script src="js/app.js"></script>-->
</head>

<body>

<div class="container">

    <!-- Title -->
    <div class="row" style="text-align:center">
        <div class="col">
            <h1 style="margin-bottom:1.3rem;">Map-free Visual Relocalization:<br/>
                Metric Pose Relative to a Single Image</h1>
            <h4 style="font-size:1.7em;">ECCV 2024 Workshop & Challenge</h4>
        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Links external -->
    <div class="row" style="text-align:center; margin-top:1rem;">
        <div class="col-12">
            <h4>
                <a style="font-size:1.1em; text-decoration: none;"
                   href="https://research.nianticlabs.com/mapfree-reloc-benchmark/leaderboard?t=single&f=2024">
                    <nobr>Leaderboard <small>(Single Frame)</small></nobr>
                </a> &emsp;
                <a style="font-size:1.1em; text-decoration: none;"
                   href="https://research.nianticlabs.com/mapfree-reloc-benchmark/leaderboard?t=multi9&f=2024">
                    <nobr>Leaderboard <small>(Multi Frame)</small></nobr>
                </a> &emsp;
                <a style="font-size:1.1em; text-decoration: none;"
                   href="https://research.nianticlabs.com/mapfree-reloc-benchmark/dataset">
                    <nobr>Dataset</nobr>
                </a> &emsp;
                <a style="font-size:1.1em; text-decoration: none;"
                   href="https://github.com/nianticlabs/map-free-reloc#bar_chart-evaluate-your-method">
                    <nobr>Benchmark<img src="assets/images/github-mark.png"
                                        style="max-height: 1.5rem; margin-top: -0.35rem;"/></nobr>
                </a> &emsp;
                <a style="font-size:1.1em; text-decoration: none;"
                   href="https://research.nianticlabs.com/mapfree-reloc-benchmark/submit">
                    <nobr>Submit</nobr>
                </a> &emsp;
                <!--<a style="font-size:1.1em; text-decoration: none;" href="TODO">
                    <nobr>Call for papers</nobr>
                </a>-->
                <a style="font-size:1.1em; text-decoration: none;"
                   href="mailto:map-free-workshop@nianticlabs.com" target="_blank">Contact us</a>
            </h4>
        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Navbar -->
    <nav class="navbar navbar-expand-lg bg-body-tertiary sticky-top">
        <div class="container-fluid">
            <a class="navbar-brand a-link" href="#">Workshop</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                    data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                    aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        <a class="nav-link a-link" href="#Schedule">Schedule</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link a-link" href="#Location">Location</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link a-link" href="#Speakers">Speakers</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link a-link" href="#Posters">Confirmed Posters</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link a-link" href="#Accepted_Papers">Accepted Papers</a>
                    </li>
                    <li class="nav-item"><a class="nav-link a-link" href="#Winners">Challenge Winners</a></li>
                    <li class="nav-item">
                        <a class="nav-link a-link" href="#Organizers">Organizers</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link a-link" href="#Sponsors">Sponsors</a>
                    </li>
                    <!-- Dropdown -->
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle a-link" href="#" role="button" data-bs-toggle="dropdown"
                           aria-expanded="false">
                            Challenge
                        </a>
                        <ul class="dropdown-menu">
                            <li><a class="dropdown-item a-link" href="#About_the_Challenge">About the Challenge</a></li>
                            <li><a class="dropdown-item a-link" href="#Important_Dates">Important Dates</a></li>
                            <li><a class="dropdown-item a-link" href="#Prizes">Prizes</a></li>
                            <li><a class="dropdown-item a-link" href="#Call_for_Papers">Call for Papers</a></li>
                            <li><a class="dropdown-item a-link" href="#Submission_Guidelines">Submission Guidelines</a>
                            </li>
                            <li><a class="dropdown-item a-link" href="#Competition_Requirements">Competition
                                Requirements</a></li>
                            <li><a class="dropdown-item a-link" href="#FAQ">FAQ</a></li>
                        </ul>
                    </li> <!-- dropdown -->
                </ul> <!-- navbar-nav -->
            </div> <!-- collapse -->
        </div> <!-- container-fluid -->
    </nav>

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Teaser -->
    <div class="row">
        <div class="col-1"></div>
        <div class="col-10 text-center align-middle">
            <video width="100%" controls autoplay muted loop style="border-radius: 2%">
                <source src="assets/images/web_mapfree.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div> <!-- col -->
        <div class="col-1"></div>
    </div> <!-- row -->

    <!-- Abstract -->
    <div class="row">
        <div class="col-12">
            <p>
                The Map-free Visual Relocalization workshop investigates topics related to
                metric visual relocalization relative to a single reference image instead of
                relative to a map.
                This problem is of major importance to many higher level applications, such as
                Augmented/Mixed Reality, SLAM and 3D reconstruction.
                It is important now, because both industry and academia are debating whether and
                how to build HD-maps of the world for those tasks. Our community is working to
                reduce the need for such maps in the first place.
                <br/>
                <br/>
                We host the first Map-free Visual Relocalization Challenge 2024 competition with
                two tracks:
                map-free metric relative pose from a single image to a single image (proposed by
                <a href="https://github.com/nianticlabs/map-free-reloc">Arnold et al. in ECCV
                    2022</a>) and from a query sequence to a single image (new).
                While the former is a more challenging and thus interesting research topic, the
                latter represents a more realistic relocalization scenario, where the system making
                the queries may fuse information from query images and tracking poses over a short
                amount of time and baseline.
                We invite papers to be submitted to the workshop.
            </p>
        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Schedule -->
    <div class="row mt-4">
        <div class="col-12">
            <h3><a id="Schedule">Schedule</a><!--
            --><small><!--
              --><a
                    onclick="copyLinkToClipboard('Schedule');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h3>
            <hr />

            <h4>üìÖ&nbsp; &nbsp; &nbsp; On Monday, 30<sup>th</sup> September, 2024, AM <small>
                <a href="http://www.google.com/calendar/event?action=TEMPLATE&dates=20240930T070000Z/20240930T110000Z&text=Map-free%20Visual%20Relocalization%20Workshop%20at%20ECCV%202024%20&location=Allianz+MiCo+%E2%80%A2+Milano+Convention+Centre&details=Map-free%20Visual%20Relocalization%20Workshop%20at%20ECCV%202024%0Ahttps%3A%2F%2Fnianticlabs.github.io%2Fmap-free-workshop%2F2024%20%0A%0AGoogle%20Maps%20%28to%20Convention%20Center%29%3A%20https%3A%2F%2Fmaps.app.goo.gl%2Fif1ooCakSvhiEPmR7%0AGoogle%20Maps%20%28to%20Suite%206%2C%20Mezzanine%2C%20South%20Wing%29%3A%20https%3A%2F%2Fmaps.app.goo.gl%2FY4iPgEFpViEKfoYN8%0A3D%20Map%3A%20https%3A%2F%2F3dmap.micomilano.it%2F"
                   target="_blank">[Add to Google Calendar]</a>

            </small></h4>
            <table class="table table-striped">
                <thead>
                <tr>
                    <th scope="col" class="text-nowrap">Time</th>
                    <th scope="col">Event</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td class="text-nowrap">09:05 - 09:20</td>
                    <td>
                        Welcome introduction by <a href="https://www.robots.ox.ac.uk/~victor/"
                                                   style="text-decoration: none;">
                        Victor Adrian Prisacariu</a><span class="fw-lighter">, Niantic and Oxford University</span>
                    </td>
                </tr>
                <tr>
                    <td class="text-nowrap">09:25 - 09:55</td>
                    <td><a href="https://jakobengel.github.io/">Jakob Engel</a><span class="fw-lighter">, Meta</span><br/>
                        <div class="mt-1"></div>
                        <i>Spatial AI for Contextual AI</i><br/>
                        <small><span class="fw-lighter">
                            This talk focuses on Spatial AI for Contextual AI: How localization and 3D spatial scene understanding will enable the next generation of smart wearables and contextually grounded AI models. I will talk about recent research including map-free and vision-free localization for AR and smart glasses, as well as semantic scene- and user- understanding; as well as talk about some of the challenges that are left.
                        </span></small>
                    </td>
                </tr>
                <tr>
                    <td class="text-nowrap">10:00 - 10:30</td>
                    <td><a href="https://etrulls.github.io/">Eduard Trulls</a><span class="fw-lighter">, Google</span><br/>
                        <div class="mt-1"></div>
                        <i>Beyond visual positioning: how localization turned out to provide efficient training of
                            neural, semantic maps</i><br/>
                        <small><span class="fw-lighter">
                        What do you do if you built a system that incorporates the largest ground-level corpus of
                        imagery into a large-scale localization system serving thousands of queries per second? You add
                        overhead and semantic data and simultaneously re-think the problem from the ground up. In <a
                                href="https://arxiv.org/pdf/2306.05407">SNAP</a>, localization becomes merely a
                        side-task and the resulting maps start to encode scene semantics, motivating an entirely new set
                        of applications and research directions. In this talk we will give a brief overview of how we
                        moved away from further iterations on Google's Visual Positioning Service to break with the
                        classic visual-localization approaches. SNAP is trained only using camera poses over tens of
                        millions of StreetView images. The resulting algorithm can resolve the location of challenging
                        image queries beyond the reach of traditional methods, outperforming the state of the art in
                        localization by a large margin. More interestingly though, our neural maps encode not only
                        geometry and appearance but also high-level semantics, discovered without explicit supervision.
                        </span></small><br/>
                        <a href="https://storage.googleapis.com/niantic-lon-static/research/mapfree/eccv24/talks/iccv24-talk-trulls.pdf"
                           target="_blank">üìú From VPS to SNAP</a> (11MB)
                    </td>
                </tr>
                <tr>
                    <td class="text-nowrap">10:30 - 11:00</td>
                    <td>Coffee break and poster session</td>
                </tr>
                <tr>
                    <td class="text-nowrap">11:00 - 11:45</td>
                    <td><a href="https://europe.naverlabs.com/people_user_naverlabs/vincent-leroy/">Vincent Leroy</a><span class="fw-lighter">,
                        Naver Labs Europe</span>
                        and Single Frame challenge track winner talk<br/>
                        <div class="mt-1"></div>
                        <i>Grounding Image Matching in 3D with MASt3R</i><br/>
                        <small><span class="fw-lighter">
                            The journey from CroCo to MASt3R exemplify a significant paradigm shift in 3D vision technologies. This presentation will delve into the methodologies, innovations, and synergistic integration of these frameworks, demonstrating their impact on the field and potential future directions. The discussion aims to highlight how these advancements unify and streamline the processing of 3D visual data, offering new perspectives and capabilities in map-free visual relocalization, robotic navigation and beyond.</span></small><br/>
                        <a href="https://storage.googleapis.com/niantic-lon-static/research/mapfree/eccv24/talks/mapfree.pdf"
                           target="_blank"> üìú Grounding Image Matching in 3D with MASt∆éR</a> (200 MB)
                    </td>
                </tr>
                <tr>
                    <td class="text-nowrap">11:50 - 12:20</td>
                    <td><a href="https://tsattler.github.io/">Torsten Sattler</a><span class="fw-lighter">, CTU Prague</span><br/>
                        <div class="mt-1"></div>
                        <i>Scene Representations for Visual Localization</i><br/>
                        <small><span class="fw-lighter">
                            Visual localization is the problem of estimating the exact position and orientation from which a
                        given image was taken. Traditionally, localization approaches either used a set of images with
                        known camera poses or a sparse point cloud, obtained from Structure-from-Motion, to represent
                        the scene. In recent years, the list of available scene representations has grown considerably.
                            In this talk, we review a subset of the available representations.
                        </span></small><br/>
                        <a href="https://storage.googleapis.com/niantic-lon-static/research/mapfree/eccv24/talks/map_free_localization_workshop_sattler.pdf"
                           target="_blank"> üìú Scene Representations for Visual Localization</a> (69 MB)
                    </td>
                </tr>
                <tr>
                    <td class="text-nowrap">12:25 - 12:55</td>
                    <td><a href="https://shubhtuls.github.io/">Shubham Tulsiani</a><span class="fw-lighter">, CMU</span><br/>
                        <div class="mt-1"></div>
                        <i>Rethinking Camera Parametrization for Pose Prediction</i><br/>
                        <small><span class="fw-lighter">
                            Every student of projective geometry is taught to represent camera matrices via an extrinsic and
                        intrinsic matrix and learning-based methods that seek to predict viewpoints given a set of
                        images typically adopt this (global) representation. In this talk, I will advocate for an
                        over-parametrized local representation which represents cameras via rays (or endpoints)
                        associated with each image pixel. Leveraging a diffusion based model that allows handling
                        uncertainty, I will show that such representations are more suitable for neural learning and
                        lead to more accurate camera prediction.
                        </span></small><br/>
                        <a href="https://storage.googleapis.com/niantic-lon-static/research/mapfree/eccv24/talks/eccv_pose.pdf"
                           target="_blank">üìú Rethinking Camera Parametrization for (Sparse-view) Pose Prediction</a> (66 MB)
                    </td>
                </tr>
                <tr>
                    <td class="text-nowrap">12:55 - 13:00</td>
                    <td>
                        <div class="row">
                            <div class="col-4">Closing Remarks and award photos</div>
                            <div class="col-8"></div>
                        </div> <!-- row -->
                        <div class="row">
                            <div class="col-3"></div>
                            <div class="col-6">
                                <div class="ratio ratio-1x1 text-center align-middle">
                                    <iframe class="yourClass"
                                            style="border: 0; width: calc(100% - 0px); height: calc(100% - 0px);"
                                            allowfullscreen
                                            src="https://scaniverse.com/scan/qafjd2g35pd7awe4?embed=1">
                                        Image of the four prize cups.
                                    </iframe>
                                </div> <!-- ratio -->
                            </div> <!-- col-6 -->
                            <div class="col-3"></div>
                        </div> <!-- row -->
                    </td> <!-- closing remarks -->
                </tr>
                </tbody>
            </table> <!-- schedule -->
        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Speakers -->
    <div class="row mt-4">
        <div class="col-12">
            <h3><a id="Speakers">Speakers</a><!--
            --><small><!--
              --><a
                    onclick="copyLinkToClipboard('Speakers');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h3>
            <hr />

            <div class="speaker" style="margin-bottom:3rem;">
                <img src="assets/speakers/jakob.jpeg" alt="Speaker Image" class="speaker-image"/>
                <div class="speaker-info">
                    <h4 class="speaker-name">
                        <a href="https://jakobengel.github.io/"
                           style="text-decoration: none;" id="Speakers__Jakob_Engel">
                            Jakob Engel<!--
                     --></a>, Meta
                    </h4>
                    <p class="speaker-bio">
                        <span class="talk-title">Talk title</span>: Spatial AI for Contextual AI <br/>
                        <span class="talk-title">Summary</span>: This talk focuses on Spatial AI for Contextual AI: How
                        localization and 3D spatial scene understanding will enable the next generation of smart
                        wearables and contextually grounded AI models. I will talk about recent research including
                        map-free and vision-free localization for AR and smart glasses, as well as semantic scene- and
                        user- understanding; as well as talk about some of the challenges that are left.
                    </p>
                    <p class="speaker-bio">
                        <span class="talk-title">Bio</span>: Jakob Engel is a Director of Research at Meta Reality labs,
                        where he is leading egocentric machine perception research as part of Meta‚Äôs Project Aria. He
                        has 10+ years of experience working on SLAM, 3D scene understanding and user/environment
                        interaction tracking, leading both research projects as well as shipping core localization
                        technology into Meta‚Äôs MR and VR product lines. Dr. Engel received his Ph.D. in Computer Science
                        at the Computer Vision Group at the Technical University of Munich in 2016, where he pioneered
                        direct methods for SLAM through DSO and LSD-SLAM.
                    </p>
                </div>
            </div> <!-- speaker -->

            <div class="speaker" style="margin-bottom:3rem;">
                <img src="assets/speakers/vincent.png" alt="Speaker Image" class="speaker-image"/>
                <div class="speaker-info">
                    <h4 class="speaker-name">
                        <a href="https://europe.naverlabs.com/people_user_naverlabs/vincent-leroy/"
                           style="text-decoration: none;" id="Speakers__Vincent_Leroy">
                            Vincent Leroy<!--
                     --></a>, Naver Labs Europe
                    </h4>
                    <p class="speaker-bio">
                        <span class="talk-title">Talk title</span>: Grounding Image Matching in 3D with MASt3R<br/>
                        <span class="talk-title">Slides</span>:
                        <a href="https://storage.googleapis.com/niantic-lon-static/research/mapfree/eccv24/talks/mapfree.pdf"
                           target="_blank">
                            üìú Grounding Image Matching in 3D with MASt∆éR
                        </a> (200 MB)<br/>
                        <span class="talk-title">Summary</span>: The journey from CroCo to MASt3R exemplify a
                        significant paradigm shift in 3D vision technologies. This presentation will delve into the
                        methodologies, innovations, and synergistic integration of these frameworks, demonstrating their
                        impact on the field and potential future directions. The discussion aims to highlight how these
                        advancements unify and streamline the processing of 3D visual data, offering new perspectives
                        and capabilities in map-free visual relocalization, robotic navigation and beyond.
                    </p>
                    <p class="speaker-bio">
                        <span class="talk-title">Bio</span>: Vincent is a research scientist in Geometric Deep Learning
                        at Naver Labs Europe.
                        He joined 5 years ago, in 2019, after completing his PhD on Multi-View Stereo Reconstruction for
                        dynamic shapes at the INRIA Grenoble-Alpes under the supervision of E. Boyer and J-S. Franco.
                        Other than that, he likes hiking in the mountains and finding simple solutions to complex
                        problems.
                        Interestingly, the latter usually comes with the former.
                    </p>
                </div>
            </div> <!-- speaker -->

            <div class="speaker" style="margin-bottom:3rem;">
                <img src="assets/speakers/torsten.jpg" alt="Speaker Image" class="speaker-image"/>
                <div class="speaker-info">
                    <h4 class="speaker-name">
                        <a href="https://tsattler.github.io/"
                           style="text-decoration: none;" id="Speakers__Torsten_Sattler">
                            Torsten Sattler<!--
                     --></a>, CTU Prague
                    </h4>
                    <p class="speaker-bio">
                        <span class="talk-title">Talk title</span>: Scene Representations for Visual Localization <br/>
                        <span class="talk-title">Slides</span>:
                        <a href="https://storage.googleapis.com/niantic-lon-static/research/mapfree/eccv24/talks/map_free_localization_workshop_sattler.pdf"
                           target="_blank">üìú Scene Representations for Visual Localization</a> (69 MB) <br/>
                        <span class="talk-title">Summary</span>: Visual localization is the problem of estimating the
                        exact position and orientation from which a given image was taken. Traditionally, localization
                        approaches either used a set of images with known camera poses or a sparse point cloud, obtained
                        from Structure-from-Motion, to represent the scene. In recent years, the list of available scene
                        representations has grown considerably. In this talk, we review a subset of the available
                        representations.
                    </p>
                    <p class="speaker-bio">
                        <span class="talk-title">Bio</span>: Torsten Sattler is a Senior Researcher at CTU. Before, he
                        was a tenured associate professor at Chalmers University of Technology. He received a PhD in
                        Computer Science from RWTH Aachen University, Germany, in 2014. From Dec. 2013 to Dec. 2018, he
                        was a post-doctoral and senior researcher at ETH Zurich. Torsten has worked on feature-based
                        localization methods [PAMI‚Äô17], long-term localization [CVPR‚Äô18, ICCV‚Äô19, ECCV‚Äô20, CVPR‚Äô21] (see
                        also the benchmarks at <a href="https://visuallocalization.net" target="_blank">visuallocalization.net</a>),
                        localization on mobile devices [ECCV‚Äô14, IJRR‚Äô20], and using semantic scene understanding for
                        localization [CVPR‚Äô18, ECCV‚Äô18, ICCV‚Äô19]. Torsten has co-organized tutorials and workshops at
                        CVPR (‚Äô14, ‚Äô15, ‚Äô17-‚Äô20), ECCV (‚Äô18, ‚Äô20), and ICCV (‚Äô17, ‚Äô19), and was / is an area chair for
                        CVPR (‚Äô18, ‚Äô22, ‚Äô23), ICCV (‚Äô21, ‚Äô23), 3DV (‚Äô18-‚Äô21), GCPR (‚Äô19, ‚Äô21), ICRA (‚Äô19, ‚Äô20), and ECCV
                        (‚Äô20). He was a program chair for DAGM GCPR‚Äô20, a general chair for 3DV‚Äô22, and will be a
                        program chair for ECCV‚Äô24.
                    </p>
                </div>
            </div> <!-- speaker -->

            <div class="speaker" style="margin-bottom:3rem;">
                <img src="assets/speakers/eduard.png" alt="Speaker Image" class="speaker-image"/>
                <div class="speaker-info">
                    <h4 class="speaker-name">
                        <a href="https://etrulls.github.io/"
                           style="text-decoration: none;" id="Speakers__Eduard_Trulls">
                            Eduard Trulls<!--
                     --></a>, Google
                    </h4>
                    <p class="speaker-bio">
                        <span class="talk-title">Talk title</span>: Beyond visual positioning: how localization turned
                        out to provide efficient training of neural, semantic maps<br/>
                        <span class="talk-title">Slides</span>:
                        <a href="https://storage.googleapis.com/niantic-lon-static/research/mapfree/eccv24/talks/iccv24-talk-trulls.pdf"
                           target="_blank">üìú From VPS to SNAP</a> (11MB) <br/>
                        <span class="talk-title">Summary</span>: What do you do if you built a system that incorporates
                        the largest ground-level corpus of imagery into a large-scale localization system serving
                        thousands of queries per second? You add overhead and semantic data and simultaneously re-think
                        the problem from the ground up. In <a href="https://arxiv.org/pdf/2306.05407">SNAP</a>,
                        localization becomes merely a side-task and the resulting maps start to encode scene semantics,
                        motivating an entirely new set of applications and research directions. In this talk we will
                        give a brief overview of how we moved away from further iterations on Google's Visual
                        Positioning Service to break with the classic visual-localization approaches. SNAP is trained
                        only using camera poses over tens of millions of StreetView images. The resulting algorithm can
                        resolve the location of challenging image queries beyond the reach of traditional methods,
                        outperforming the state of the art in localization by a large margin. More interestingly though,
                        our neural maps encode not only geometry and appearance but also high-level semantics,
                        discovered without explicit supervision.
                    </p>
                    <p class="speaker-bio">
                        <span class="talk-title">Bio</span>: Eduard Trulls is a Research Scientist at Google Zurich,
                        working on Machine Learning for visual recognition. Before that he was a post-doc at the
                        Computer Vision Lab at EPFL in Lausanne, Switzerland, working with Pascal Fua. He obtained his
                        PhD from the Institute of Robotics in Barcelona, Spain, co-advised by Francesc Moreno and
                        Alberto Sanfeliu. Before his PhD he worked in mobile robotics.
                    </p>
                    <p>
                    </p>
                </div>
            </div> <!-- speaker -->

            <div class="speaker" style="margin-bottom:3rem;">
                <img src="assets/speakers/shubham.jpg" alt="Speaker Image" class="speaker-image"/>
                <div class="speaker-info">
                    <h4 class="speaker-name">
                        <a href="https://shubhtuls.github.io/"
                           style="text-decoration: none;" id="Speakers__Shubham_Tulsiani">
                            Shubham Tulsiani<!--
                     --></a>, Carnegie Mellon University
                    </h4>
                    <p class="speaker-bio">
                        <span class="talk-title">Talk title</span>: Rethinking Camera Parametrization for Pose
                        Prediction<br/>
                        <span class="talk-title">Slides</span>:<a
                            href="https://storage.googleapis.com/niantic-lon-static/research/mapfree/eccv24/talks/eccv_pose.pdf"
                            target="_blank">üìú Rethinking Camera Parametrization for (Sparse-view) Pose Prediction</a>
                        (66 MB)<br/>
                        <span class="talk-title">Summary</span>: Every student of projective geometry is taught to
                        represent camera matrices via an extrinsic and intrinsic matrix and learning-based methods that
                        seek to predict viewpoints given a set of images typically adopt this (global) representation.
                        In this talk, I will advocate for an over-parametrized local representation which represents
                        cameras via rays (or endpoints) associated with each image pixel. Leveraging a diffusion based
                        model that allows handling uncertainty, I will show that such representations are more suitable
                        for neural learning and lead to more accurate camera prediction.
                    </p>
                    <p class="speaker-bio">
                        <span class="talk-title">Bio</span>: Shubham Tulsiani is an Assistant Professor at Carnegie
                        Mellon University in the Robotics Institute. Prior to this, he was a research scientist at
                        Facebook AI Research (FAIR). He received a PhD. in Computer Science from UC Berkeley in 2018
                        where his work was supported by the Berkeley Fellowship. He is interested in building perception
                        systems that can infer the spatial and physical structure of the world they observe. He was the
                        recipient of the Best Student Paper Award in CVPR 2015.
                    </p>
                </div>
            </div> <!-- speaker -->

            <div class="speaker" style="margin-bottom:3rem;">
                <img src="assets/organizers/victor.jpg" alt="Speaker Image" class="speaker-image"/>
                <div class="speaker-info">
                    <h4 class="speaker-name">
                        <a href="https://www.robots.ox.ac.uk/~victor/"
                           style="text-decoration: none;" id="Speakers__Victor_Adrian_Prisacariu">
                            Victor Adrian Prisacariu<!--
                     --></a>, Niantic and Oxford University
                    </h4>
                    <p class="speaker-bio">
                        <span class="talk-title">Talk title</span>: TBC (opening remarks)
                    </p>
                    <p class="speaker-bio">
                        <span class="talk-title">Bio</span>: Professor Victor Adrian Prisacariu received the Graduate
                        degree (with first class hons.) in computer engineering from Gheorghe Asachi Technical
                        University, Iasi, Romania, in 2008, and the D.Phil. degree in engineering science from the
                        University of Oxford in 2012.<br/>
                        He continued here first as an EPSRC prize Postdoctoral
                        Researcher, and then as a Dyson Senior Research Fellow, before being appointed an Associate
                        Professor in 2017. <br/>
                        He also co-founded <a href="https://6D.ai">6D.ai</a>, where he built
                        APIs to help developers augment reality in ways that users would find meaningful, useful and
                        exciting. The 6D.ai SDK used a standard built-in smartphone camera to build a cloud-based,
                        crowdsourced three-dimensional semantic map of the world all in real-time, in the background.
                        6D.ai was acquired by Niantic in March 2020. He is now Chief Scientist with Niantic. <br/>
                        Victor's research interests include semantic visual tracking, 3-D reconstruction, and SLAM.
                    </p>
                </div>
            </div> <!-- speaker -->

        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Posters -->
    <div class="row mt-4">
        <div class="col-12">
            <h4><a id="Posters">Confirmed posters</a><!--
            --><small><!--
              --><a
                    onclick="copyLinkToClipboard('Posters');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h4>
            <hr />
            <ul>
                <li>
                    <a href="https://arxiv.org/abs/2406.09756">Grounding Image Matching in 3D with MASt3R</a><br />
                    Vincent Leroy, Yohann Cabon and J√©r√¥me Revaud <br />
                    <a href="https://europe.naverlabs.com/">Naver Labs Europe</a>
                </li>
                <li><a href="assets/Map_Free_Challenge_DepthMickey.pdf">Improving Map-Free Localization with Depth Supervision</a><br />
                    Hitesh Jain and Sagar Verma <br />
                    <a href="https://www.granular.ai/">Granular AI</a>
                </li>
                <li>
                    <a href="https://github.com/surgical-vision/colmap-match-converter">Mismatched: Evaluating the
                        Limits of Image Matching Approaches and Benchmarks</a><br/>
                    Sierra Bonilla, Chiara Di Vece, Rema Daher, Xinwei Ju, Danail Stoyanov, Francisco Vasconcelos,
                    Sophia Bano <br/>
                    <a href="https://www.ucl.ac.uk/interventional-surgical-sciences/wellcome-epsrc-centre-interventional-and-surgical-sciences-weiss">University
                        College London</a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2403.08156">NeRF-Supervised Feature Point Detection and Description</a><br />
                    Ali Youssef, Fransisco Vasconcelos<br />
                    <a href="https://www.ucl.ac.uk/computer-science/">University College London</a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2404.14409">CrossScore: Towards Multi-View Image Evaluation and Scoring</a><br />
                    Zirui Wang, Wenjing Bian, Victor Adrian Prisacariu <br />
                    <a href="https://crossscore.active.vision/">University of Oxford</a>
                </li>
            </ul>
        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Winners -->
    <div class="row mt-4">
        <div class="col-12">
            <h3><a id="Winners">üèÜ 2024 Challenge Winners</a><!--
            --><small><!--
              --><a onclick="copyLinkToClipboard('Winners');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h3>
            <hr/>
            <h5>Single Frame <small>(<a
                    href="https://research.nianticlabs.com/mapfree-reloc-benchmark/leaderboard?t=single&f=2024">Leaderboard</a>)</small>
            </h5>
            <div class="row">
                <div class="col-1" style="margin-top: -0.8em;">
                    <figure class="figure">
                        <span class="emoji-large">ü•á</span>
                        <figcaption class="figure-caption text-center" id="figcaption-first-place">
                            <!-- $2,000 -->
                        </figcaption>
                    </figure>
                </div>
                <div class="col-4">
                    <span class="fw-bolder"><em>Grounding Image Matching in 3D with MASt3R</em></span><br/>
                    &emsp;<span class="fw-lighter">"<i>MASt3R (Ess.Mat + D.Scale)</i>"</span>
                </div>
                <div class="col-7">
                    <div class="div-hanging-indent">
                        Vincent Leroy, Yohann Cabon and Jerome Revaud
                    </div>
                    <div class="div-hanging-indent">
                        Naver Labs Europe
                    </div>
                    <a href="https://arxiv.org/abs/2406.09756">https://arxiv.org/abs/2406.09756</a>
                </div> <!-- col-7 -->
            </div> <!-- row -->
            <div class="row mt-1">
                <div class="col-1" style="margin-top: -0.8em;">
                    <figure class="figure">
                        <span class="emoji-large">ü•à</span>
                        <figcaption class="figure-caption text-center" id="figcaption-second-place">
                            <!-- $1,600 -->
                        </figcaption>
                    </figure>
                </div> <!-- col-1 -->
                <div class="col-4">
                    <span class="fw-bolder"><em>Metric Pose Estimation Relative to Anchor Frame for Map-free Localization</em></span><br/>
                    &emsp;<span class="fw-lighter">"<i>interp_metric3d_loftr_3d2d</i>"</span>
                </div>
                <div class="col-7">
                    <div class="div-hanging-indent">
                        Lili Zhao<sup>1</sup>, Zhili Liu<sup>2</sup>, Lei Yang<sup>1</sup>, and Meng Guo<sup>1</sup>
                    </div>
                    <div class="div-hanging-indent">
                        <sup>1</sup>China Mobile Research Institute,
                    </div>
                    <div class="div-hanging-indent">
                        <sup>2</sup>N/A <br/>
                    </div>
                    <a href="https://github.com/Shepherddot/An-Implementation-For-Two-View-Metric-Pose-Estimation">https://github.com/Shepherddot/An-Implementation-For-Two-View-Metric-Pose-Estimation</a>
                </div> <!-- col-7 -->
            </div> <!-- row -->
            <div class="row mt-1">
                <div class="col-1" style="margin-top: -0.8em;">
                    <figure class="figure">
                        <span class="emoji-large">ü•â</span>
                        <figcaption class="figure-caption text-center" id="figcaption-third-place">
                            <!-- $1,400 -->
                        </figcaption>
                    </figure>
                </div> <!-- col-1 -->
                <div class="col-4">
                    <span class="fw-bolder"><em>Map-Free Visual Relocalization Enhanced by Instance Knowledge and Depth Knowledge</em></span>
                </div> <!-- col-4 -->
                <div class="col-7">
                    <div class="div-hanging-indent">
                        Mingyu Xiao<sup>1</sup>, Runze Chen <sup>1</sup>, Haiyong Luo<sup>2</sup>, Fang Zhao<sup>1</sup>,
                        Juan Wang<sup>3</sup>, and Xue Peng Ma<sup>3</sup>
                    </div>
                    <div class="div-hanging-indent">
                        <sup>1</sup>Beijing University of Posts and Telecommunications,
                    </div> <!-- hanging indent -->
                    <div class="div-hanging-indent">
                        <sup>2</sup>Research Center for Ubiquitous Computing Systems, Institute of Computing Technology,
                        Chinese Academy of Sciences
                    </div> <!-- hanging indent -->
                    <div class="div-hanging-indent">
                        <sup>3</sup>Shouguang Cheng Zhi Feng Xing Technology Co., Ltd.
                    </div> <!-- hanging indent -->
                    <a href="http://arxiv.org/abs/2408.13085">http://arxiv.org/abs/2408.13085</a>
                </div>
            </div> <!-- row -->
            <div class="row mt-1">
                <div class="col-1 align-middle" style="margin-top: 0.45em;">
                    <figure class="figure">
                        <img style="width: 8.0vw; max-width: 80px;" src="assets/images/4th-place-medal-low.png"/>
                        <figcaption class="figure-caption text-center" id="figcaption-fourth-place">
                            <!-- $1,000 -->
                        </figcaption>
                    </figure>
                </div> <!-- col-1 -->
                <div class="col-4">
                    <span class="fw-bolder"><em>Improving Map-Free Localization with Depth Supervision</em></span><br/>
                    &emsp;<span class="fw-lighter">"<i>Mickey Variant GT_Depth</i>"</span>
                </div><!-- col-4 -->
                <div class="col-7">
                    <div class="div-hanging-indent">
                        Hitesh Jain and Sagar Verma
                    </div>
                    <div class="div-hanging-indent">
                        Granular AI
                    </div>
                    <a href="https://hal.science/hal-04696742v1/document">https://hal.science/hal-04696742v1/document</a>
                </div>
            </div> <!-- row -->
            <div class="row mt-2">
                <div class="col-12">
                    <h5>Multi Frame <small>(<a
                            href="https://research.nianticlabs.com/mapfree-reloc-benchmark/leaderboard?t=multi&f=2024">Leaderboard</a>)</small>
                    </h5>
                    No accepted entries. <br/>
                </div> <!-- col -->
            </div> <!-- row -->
        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Prizes -->
    <div class="row mt-4">
        <div class="col-12">
            <h3><a id="Prizes">Prizes</a><!--
            --><small><!--
              --><a onclick="copyLinkToClipboard('Prizes');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h3>
            <hr/>
            <p>
                <span class="fs-5">$6000 in prizes will be divided between the top submissions of the two tracks.</span><br/>
                Niantic is also seeking partners from the growing community to co-fund and co-judge
                the prizes.
            </p>
            <div class="blockquote">
                <span class="text-light badge bg-gradient" style="background-color: #df471c">Update</span>
                <a href="https://europe.naverlabs.com">Naver Labs Europe</a> has kindly agreed to co-sponsor the
                challenge with $2000!
            </div>
        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- About the Challenge -->
    <div class="row mt-4">
        <div class="col-12">
            <h3><a id="About_the_Challenge">About the Challenge</a><!--
            --><small><!--
              --><a
                    onclick="copyLinkToClipboard('About_the_Challenge');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h3>
            <hr />
            <p>
                We have extended the Map-free benchmark for the challenge with a sequence-based
                scenario, based on feedback from senior community members.
                Therefore, the challenge consists of two tracks:<br/>
                &nbsp;&nbsp; 1. The original, <strong>single query frame</strong> to single map
                frame task published
                with the <a
                    href="https://storage.googleapis.com/niantic-lon-static/research/map-free-reloc/MapFreeReloc-ECCV22-paper.pdf"
                    target="_blank">ECCV 2022 paper</a>;<br/>
                &nbsp;&nbsp; 2. A new task with <strong>multiple query frames</strong> (9) and their
                mobile device provided, metric tracking poses.
            </p>
            <h5> 1. Single query frame to a single "map" frame</h5>
            <p>
                To recap, the task in the first track consists of from a single query image predict
                the metric relative pose to a single map image without any further auxiliary
                information.
            </p>
            <h5> 2. Multiple query frames (9) to a single "map" frame</h5>
            <p>
                The second track is motivated by the observation that a burst of images, capturing
                small motion, can be recorded while staying true to the map-free scenario: No
                significant data capture or exploration of the environment.<br/>
                At the same time, the burst of images allows the application of multi-frame depth
                estimation and contains strong hints about the scene scale from the IMU sensor on
                device.<br/>
                We created a second version of the test set and <a
                    href="https://research.nianticlabs.com/mapfree-reloc-benchmark/leaderboard?t=multi9&f=2024">leaderboard</a>
                for this track.
            </p>
        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Contact form -->
    <div class="row">
        <div class="col-12">
            <h4><a id="Contact_form">Stay up to date!</a><!--
            --><small><!--
              --><a onclick="copyLinkToClipboard('Contact_form');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h4>
            <hr />
            <p>Please register your interest
                <a href="https://docs.google.com/forms/d/e/1FAIpQLScMKpHBn_fPrFRSpVp9BtiH3DbNRM4pj_8kjWVx-M1SQFrt0w/viewform?usp=sf_link">
                    here</a>, so we can keep you notified about news and updates!</p>
        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Important Dates -->
    <div class="row mt-4">
        <div class="col-12">
            <h3><a id="Important_Dates">Important Dates</a><!--
            --><small><!--
              --><a
                    onclick="copyLinkToClipboard('Important_Dates');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h3>
            <hr />
            <dl class="row">
                <dt class="col-5">
                    Challenge start
                </dt>
                <dd class="col-7">
                    <ul class="list-inline">
                        <!-- note: if you change the date, please change it at the bottom <script> as well -->
                        <li class="list-inline-item">21<sup>st</sup> May, 2024</li>
                        <li class="list-inline-item"><span id="day-start-countdown" class="mx-4"></span></li>
                    </ul>
                </dd>

                <dt class="col-5">
                    <span class="workshop-paper">Workshop paper</span> and
                    <span class="extended-abstract">extended abstracts</span> submission deadline
                </dt>
                <dd class="col-7">
                    <ul class="list-inline">
                        <!-- note: if you change the date, please change it at the bottom <script> as well -->
                        <li class="list-inline-item">&nbsp;2<sup>nd</sup> August, 2024</li>
                        <li class="list-inline-item"><span id="day-paper-submit-countdown"></span></li>
                    </ul>
                </dd>

                <dt class="col-5">
                    <span class="workshop-paper">Workshop paper</span> and
                    <span class="extended-abstract">extended abstracts</span> camera-ready deadline
                </dt>
                <dd class="col-7">
                    <ul class="list-inline">
                        <!-- note: if you change the date, please change it at the bottom <script> as well -->
                        <li class="list-inline-item">22<sup>nd</sup> August, 2024</li>
                        <li class="list-inline-item"><span id="day-paper-camera-ready-countdown"></span></li>
                    </ul>
                </dd>

                <dt class="col-5">Challenge end</dt>
                <dd class="col-7">
                    <ul class="list-inline">
                        <!-- note: if you change the date, please change it at the bottom <script> as well -->
                        <li class="list-inline-item">23<sup>rd</sup> August, 2024</li>
                        <li class="list-inline-item"><span id="day-end-countdown"></span></li>
                    </ul>
                </dd>

                <dt class="col-5">
                    Challenge submission <span class="method-description">method description</span> deadline
                </dt>
                <dd class="col-7">
                    <ul class="list-inline">
                        <!-- note: if you change the date, please change it at the bottom <script> as well -->
                        <li class="list-inline-item">27<sup>th</sup> August, 2024</li>
                        <li class="list-inline-item"><span id="day-describe-countdown"></span></li>
                    </ul>
                </dd>

                <dt class="col-5">Challenge winners announcement</dt>
                <dd class="col-7">
                    <ul class="list-inline">
                        <!-- note: if you change the date, please change it at the bottom <script> as well -->
                        <li class="list-inline-item">30<sup>th</sup> August, 2024</li>
                        <li class="list-inline-item"><span id="day-winners-countdown"></span></li>
                    </ul>
                </dd>

                <dt class="col-5">Workshop date (ECCV'24)</dt>
                <dd class="col-7">
                    <ul class="list-inline">
                        <!-- note: if you change the date, please change it at the bottom <script> as well -->
                        <li class="list-inline-item">30<sup>th</sup> September, 2024, AM</li>
                        <li class="list-inline-item"><span id="day-workshop-countdown" class="mb-4"></span></li>
                    </ul>
                    <div style="margin-top: -1.1rem;">
                        <small>
                            <a href="http://www.google.com/calendar/event?action=TEMPLATE&dates=20240930T070000Z/20240930T110000Z&text=Map-free%20Visual%20Relocalization%20Workshop%20at%20ECCV%202024%20&location=Allianz+MiCo+%E2%80%A2+Milano+Convention+Centre&details=Map-free%20Visual%20Relocalization%20Workshop%20at%20ECCV%202024%0Ahttps%3A%2F%2Fnianticlabs.github.io%2Fmap-free-workshop%2F2024%20%0A%0AGoogle%20Maps%20%28to%20Convention%20Center%29%3A%20https%3A%2F%2Fmaps.app.goo.gl%2Fif1ooCakSvhiEPmR7%0AGoogle%20Maps%20%28to%20Suite%206%2C%20Mezzanine%2C%20South%20Wing%29%3A%20https%3A%2F%2Fmaps.app.goo.gl%2FY4iPgEFpViEKfoYN8%0A3D%20Map%3A%20https%3A%2F%2F3dmap.micomilano.it%2F"
                               target="_blank">[Add to Google Calendar]</a>
                        </small>
                    </div>
                </dd>
            </dl>
        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Location -->
    <div class="row mt-4">
        <div class="col-12">
            <h4><a id="Location">Location</a><!--
            --><small><!--
              --><a
                    onclick="copyLinkToClipboard('Location');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h4>
            <hr/>
            <h4>üè§üìç Suite 6, South Level Mezzanine &nbsp;&nbsp;&nbsp;<small><a
                    href="https://maps.app.goo.gl/Y4iPgEFpViEKfoYN8" target="_blank">Google Maps to Suite 6, Mezzanine,
                South Wing</a></small></h4>
            <a href="https://www.google.com/maps/place/Allianz+MiCo+%E2%80%A2+Milano+Convention+Centre/@45.4813272,9.1528774,17z/data=!3m2!4b1!5s0x4786c10d38e671eb:0x1e3146974449414c!4m6!3m5!1s0x4786c112eaeffe77:0x6c1c03719d72cec5!8m2!3d45.4813235!4d9.1554577!16s%2Fg%2F11cs37h0p8?entry=ttu">Allianz
                MiCo ‚Ä¢ Milano Convention Centre</a><br/>
            Viale Eginardo - Gate 2 (Or Piazzale Carlo Magno) ‚Äì Gate 16 (for those arriving from Domodossola Metro
            Station) <br/>
            20149 Milano, Italy
        </div> <!-- col -->
    </div> <!-- row -->

    <div class="row">
        <div class="col-5 align-middle text-center">
            <figure class="figure">
                <a href="assets/images/Allianz%20MiCo_Level_2M.png">
                    <img width="100%" src="assets/images/Allianz%20MiCo_Level_2M.png"/>
                </a>
                <figcaption class="figure-caption">
                    Suite 6, Mezzanine, South Wing
                </figcaption>
            </figure>
        </div> <!-- col6 -->
        <div class="col-7 align-middle text-center">
            <figure class="figure">
                <a href="https://3dmap.micomilano.it/" target="_blank">
                    <img src="assets/images/MiCo3DMap.gif" style="height: 100%;"/>
                </a>
                <figcaption class="figure-caption">
                    <a href="https://3dmap.micomilano.it/">https://3dmap.micomilano.it/</a>
                </figcaption>
            </figure>
        </div> <!-- col6 -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Call for Papers -->
    <div class="row mt-4">
        <div class="col-12">
            <h3><a id="Call_for_Papers">Call for Papers: ECCV Map-free Visual Relocalization
                Workshop & Challenge 2024</a><!--
            --><small><!--
              --><a onclick="copyLinkToClipboard('Call_for_Papers');"
                    style="cursor: pointer;"> üîó<!--
                    --></a><!--
                  --></small><!--
               --></h3>
            <hr />
            <p>We invite submissions of <span class="workshop-paper">workshop papers</span> and <span class="extended-abstract">extended abstracts</span> to the ECCV Map-free Visual Relocalization
                Workshop & Challenge 2024.
                This workshop aims to advance the field of visual relocalization without relying on pre-built maps.
                The following topics and related areas are of interest:</p>
                <div class="row">
                    <div class="col-6">
                        <ul>
                            <li>visual relocalization,</li>
                            <li>feature-matching,</li>
                            <li>pose regression (absolute and relative),</li>
                            <li>depth estimation (monocular and multi-frame),</li>
                        </ul>
                    </div>
                    <div class="col-6">
                        <ul>
                            <li>scale estimation,</li>
                            <li>confidence and uncertainty,</li>
                            <li>structure-from-motion</li>
                        </ul>
                    </div>
                </div>
            <p><span class="workshop-paper">Workshop paper</span> and <span class="extended-abstract">extended abstract</span> submission deadline: 2<sup>nd</sup> August, 2024. See also <a href="#Important_Dates">Important Dates</a>.<br/>
            Sign up through the <a href="#Contact_form">contact form</a> to stay up to date with future announcements.</p>
        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Accepted Papers -->
    <div class="row mt-4">
        <div class="col-12">
            <h4><a id="Accepted_Papers">üèÜ Accepted Papers (pre-prints!)</a><!--
            --><small><!--
              --><a onclick="copyLinkToClipboard('Accepted_Papers');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h4>
            <hr />
            Here follows a list of pre-prints of the three <span class="workshop-paper">workshop papers</span> accepted to the ECCV Map-free Visual Relocalization Workshop & Challenge 2024:
            <ul>
                <li>
                    <a href="https://arxiv.org/abs/2408.16445v1">Mismatched: Evaluating the Limits of Image Matching Approaches and Benchmarks</a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2403.08156">NeRF-Supervised Feature Point Detection and Description</a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2404.09271">VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field</a>
                </li>
            </ul>

        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Submission Guidelines -->
    <div class="row mt-4">
        <div class="col-12">
            <h4><a id="Submission_Guidelines">Submission Guidelines</a><!--
            --><small><!--
              --><a onclick="copyLinkToClipboard('Submission_Guidelines');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h4>
            <hr />

            <div class="row">
                <div class="col">
                    <p>
                        Camera ready versions of accepted <span class="workshop-paper">workshop papers</span> should be
                        submitted in the ECCV main conference camera ready template for your workshop papers.
                        The template can be accessed <a href="https://www.overleaf.com/read/gqtmnkdjsrwq#102a58">
                        here</a> and <a href="https://eccv.ecva.net/Conferences/2024/SubmissionPolicies">here</a>.
                    </p>
                </div> <!-- col -->
            </div> <!-- row -->

            <div class="row">
                <div class="col-md-4">
                    <h5>
                        1. <span id="Workshop_paper" class="workshop-paper">Workshop paper</span><!--
                     --><small><!--
                         --><a onclick="copyLinkToClipboard('Workshop_paper');"
                               style="cursor: pointer;"> üîó<!--
                         --></a><!--
                     --></small><!--
                 --></h5>
                    <ul>
                        <li>Deadline: 2<sup>nd</sup> August, 2024.</li>
                        <li>Follow the ECCV <a href="https://eccv.ecva.net/Conferences/2024/SubmissionPolicies">paper
                            submission policies</a>.
                        </li>
                        <li>Use the same format as for submissions to the main conference: <a
                                href="https://www.overleaf.com/read/hdqrbdhdnmrv#d6ad4f">official ECCV 2024 template</a>.
                        </li>
                        <li>Accepted <span class="workshop-paper">workshop papers</span> should follow the ECCV paper chairs' guidance. <br/> They will most likely
                            communicate to use the <a
                                    href="https://www.overleaf.com/latex/templates/springer-lecture-notes-in-computer-science/kzwwpvhwnvfj#.WuA4JS5uZpi">official
                                Springer ECCV 2024 template</a> for the camera-ready version.
                        </li>
                        <li>Evaluation on the <a
                                href="https://research.nianticlabs.com/mapfree-reloc-benchmark/dataset">Niantic Map-free
                            Relocalization Dataset</a> is encouraged but not required.
                        </li>
                        <li> Please submit at the workshop <a
                                href="https://cmt3.research.microsoft.com/MAPFREEWorkshop2024/Submission/Index">CMT
                            submission portal</a>.<br />
                            Use the "<span class="fw-light">Workshop paper track</span>" category under "Subject areas".
                        </li>
                    </ul>
                </div>
                <div class="col-md-4">
                    <h5>2. <span id="Extended_abstract" class="extended-abstract">Extended abstract</span><!--
                     --><small><!--
                         --><a onclick="copyLinkToClipboard('Extended_abstract');"
                               style="cursor: pointer;"> üîó<!--
                         --></a><!--
                     --></small><!--
                 --></h5>
                    <ul>
                        <li>Deadline: 2<sup>nd</sup> August, 2024.</li>
                        <li>
                            <span class="extended-abstract">Extended abstracts</span> are not subject to the ECCV rules, so they can be in any template (<em>e.g.</em>,
                            <a href="https://www.overleaf.com/read/hdqrbdhdnmrv#d6ad4f">ECCV</a> or
                            <a href="https://github.com/cvpr-org/author-kit/releases">CVPR</a>).
                        </li>
                        <li>The maximum length of <span class="extended-abstract">extended abstracts</span> <b>excluding</b> references is the equivalent
                            of 4 pages in the official <a href="https://github.com/cvpr-org/author-kit/releases">CVPR</a> 2024 format.<br/>
                            <!--See "Dual submissions" in the <a
                                    href="https://eccv.ecva.net/Conferences/2024/SubmissionPolicies">ECCV 2024
                                Submission Policies</a>.
                            <blockquote class="blockquote border-start border-secondary ps-3 my-3">
                                <p class="fs-6"><em>A publication, for the purposes of this policy, is defined to be a
                                    written work longer than <b>four pages (excluding references)</b> that was submitted
                                    for review by peers for either acceptance or rejection, and, after review, was
                                    accepted.</em></p>
                            </blockquote>-->
                        </li>
                        <li><span class="extended-abstract">Extended abstracts</span> can include already published or accepted works, as they are meant to
                            provide authors with the possibility to showcase their ongoing research in areas relevant to
                            map-free relocalization.
                        </li>
                        <li><span class="extended-abstract">Extended abstracts</span> will not be included in the conference proceedings, do not count as paper
                            publication.
                        </li>
                        <li>However, the authors of accepted <span class="extended-abstract">extended abstracts</span> will get the opportunity to present
                            their work at the workshop poster session.
                        </li>
                        <li>Examples of <span class="extended-abstract">extended abstracts</span>:<a
                                href="https://sites.google.com/view/hands2022/program?authuser=0#h.pxvdrcd9tn4j">
                            Accepted Extended Abstracts from HANDS @ ECCV 2022
                        </a>. <em>E.g.</em>, <br/>
                            Scalable High-fidelity 3D Hand Shape Reconstruction via Graph Frequency Decomposition.
                            Tianyu Luan, Jingjing Meng, Junsong Yuan.
                            [<a href="https://drive.google.com/file/d/1JExKfdkt4NaJ7PHS-5Pg1LbXuYyEJd08/view?usp=sharing">pdf</a>]
                        </li>
                        <li>
                            Please submit at the workshop <a
                                href="https://cmt3.research.microsoft.com/MAPFREEWorkshop2024/Submission/Index">CMT
                            submission portal</a>. <br />
                            Use the "<span class="fw-light">Extended abstract track</span>" category under "Subject areas".
                        </li>
                    </ul>
                </div>
                <div class="col-md-4">
                    <h5>3. <span id="Method_description" class="method-description">Method description</span>
                        (~technical report)<!--
                     --><small><!--
                         --><a onclick="copyLinkToClipboard('Method_description');"
                               style="cursor: pointer;"> üîó<!--
                         --></a><!--
                     --></small><!--
                 --></h5>
                    <ul>
                        <li>
                            Challenge leaderboard entry deadline: 23<sup>rd</sup> August, 2024.
                        </li>
                        <li>
                            <span class="method-description">Method description</span> publication deadline:
                            27<sup>th</sup> August, 2024.
                        </li>
                        <li>
                            The minimum required for valid competition entries.
                        </li>
                        <li>
                            Accepted <span class="workshop-paper">workshop papers</span> or accepted
                            <span class="extended-abstract">extended abstracts</span> also qualify as
                            valid <span class="method-description">method descriptions</span>, no need to submit more.
                        </li>
                        <li>
                            Must be publicly available on ArXiv or equivalent. <br/>
                            If there are delays with an ArXiv publication, please send us proof of pending submission.
                        </li>
                        <li>
                            This can be a non-peer reviewed paper in any format (<em>e.g.</em>,
                            <a href="https://www.overleaf.com/read/hdqrbdhdnmrv#d6ad4f">ECCV</a> or
                            <a href="https://github.com/cvpr-org/author-kit/releases">CVPR</a>).
                            The <span class="method-description">method description</span> has to contain sufficient
                            detail for replication of the leaderboard entry's results.
                        </li>
                    </ul>
                </div> <!-- col -->
            </div> <!-- row submission types -->
        </div> <!-- col -->
    </div> <!-- row submission guidelines -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Competition Requirements -->
    <div class="row mt-4">
        <div class="col-12">
            <h4><a id="Competition_Requirements">Competition Requirements</a><!--
            --><small><!--
            --><a onclick="copyLinkToClipboard('Competition_Requirements');"
                  style="cursor: pointer;"> üîó<!--
            --></a><!--
          --></small><!--
         --></h4>
            <hr />
            <ul>
                <li>Entries must be submitted within the competition time limits:
                    21<sup>st</sup> May, 2024 - 23<sup>rd</sup> August, 2024.
                </li>
                <li>Entries in the leaderboard that were submitted <b>before</b> the 21<sup>st</sup> May, 2024 will <b>not</b>
                    be considered as participating in the 2024 Map-free Visual Relocalization challenge.
                </li>
                <li>
                    Entries in the leaderboard must point to a valid
                    <span class="method-description">method description</span>
                    (as outlined above under "<b>3. <span class="method-description">Method description</span></b>").
                </li>
            </ul>
        </div>
    </div>

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- FAQ -->
    <div class="row mt-4">
        <div class="col-12">
            <h4><a id="FAQ">FAQ</a><!--
            --><small><!--
            --><a onclick="copyLinkToClipboard('FAQ');"
                  style="cursor: pointer;"> üîó<!--
            --></a><!--
          --></small><!--
         --></h4>
            <hr />

            <div class="accordion" id="faqAccordion">
                <div class="accordion-item">
                    <h2 class="accordion-header" id="headingOne">
                        <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                                data-bs-target="#collapseOne" aria-expanded="false" aria-controls="collapseOne">
                                <span class="accordion-button-emoji-container">üèÜ‚úÖ<span class="mx-2"></span>üßæ‚ùå</span>
                        </button>
                    </h2>
                    <div id="collapseOne" class="accordion-collapse collapse" aria-labelledby="headingOne">
                        <div class="accordion-body">
                            <dl>
                                <div class="row">
                                    <dt class="col-auto" style="width: 20px;"><b>Q</b>:</dt>
                                    <dd class="col">
                                        I would like to participate in the challenge, and I don't have a paper
                                        yet for my method. What should I do?
                                    </dd>
                                </div>
                                <div class="row">
                                    <dt class="col-auto" style="width: 20px;"><b>A</b>:</dt>
                                    <dd class="col">
                                        You should submit a <span class="workshop-paper">workshop paper</span>
                                        or an <span class="extended-abstract">extended abstract</span> to the workshop,
                                        or make a <span class="method-description">method description</span> available on ArXiv.<br/>
                                        Upon acceptance, your leaderboard entry should refer to the accepted submission.
                                        <br/>
                                        If rejected, make sure to have a <span class="method-description">method description</span>
                                        available on ArXiv to keep your leaderboard entry valid. <br/>
                                        Make sure your leaderboard entry dates between the competition start and end
                                        dates (21<sup>st</sup> May, 2024 - 23<sup>rd</sup> August, 2024).
                                    </dd>
                                </div> <!-- row -->
                            </dl>
                        </div>
                    </div>
                </div> <!-- accordion-item -->

                <!-- --------------- -->

                <div class="accordion-item">
                    <h2 class="accordion-header" id="headingTwo">
                        <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                                data-bs-target="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">
                            <span class="accordion-button-emoji-container">üèÜ‚úÖ<span class="mx-2"></span>üßæ‚úÖ</span>
                        </button>
                    </h2>
                    <div id="collapseTwo" class="accordion-collapse collapse" aria-labelledby="headingTwo">
                        <div class="accordion-body">
                            <dl>
                                <div class="row">
                                    <dt class="col-auto" style="width: 20px;"><b>Q</b>:</dt>
                                    <dd class="col">I would like to participate in the challenge with a method I have
                                        already published. What should I do?
                                    </dd>
                                </div>
                                <div class="row">
                                    <dt class="col-auto" style="width: 20px;"><b>A</b>:</dt>
                                    <dd class="col">
                                        Make sure your leaderboard entry dates between the competition start and end
                                        dates (21<sup>st</sup> May, 2024 - 23<sup>rd</sup> August, 2024). <br/>
                                        Your leaderboard entry should point to a valid method description, <em>e.g.</em>,
                                        the link to your published paper.
                                    </dd>
                                </div>
                            </dl>
                        </div>
                    </div>
                </div> <!-- accordion-item -->

                <!-- --------------- -->

                <div class="accordion-item">
                    <h2 class="accordion-header" id="headingThree">
                        <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                                data-bs-target="#collapseThree" aria-expanded="false" aria-controls="collapseThree">
                            <span class="accordion-button-emoji-container">üèÜ‚úÖ<span class="mx-2"></span>üßæ‚úÖ‚ûïüèóÔ∏è</span>
                        </button>
                    </h2>
                    <div id="collapseThree" class="accordion-collapse collapse" aria-labelledby="headingThree">
                        <!--data-bs-parent="#faqAccordion"-->
                        <div class="accordion-body">
                            <dl>
                                <div class="row">
                                    <dt class="col-auto" style="width: 20px;"><b>Q</b>:</dt>
                                    <dd class="col">
                                        I would like to participate in the challenge with a method I have already
                                        published, but I've made some changes to the method specific to the challenge.
                                        What should I do?
                                    </dd>
                                </div> <!-- row Q -->
                                <div class="row">
                                    <dt class="col-auto" style="width: 20px;"><b>A</b>:</dt>
                                    <dd class="col">You probably should submit an
                                        <span class="extended-abstract">extended abstract</span> to the workshop or
                                        make a <span class="method-description">method description</span> available on
                                        ArXiv, citing your original work. <br/>
                                        Resubmitting your original work without sufficient amount of differences
                                        compared to the already submitted or published version might violate the ECCV
                                        <a href="https://eccv.ecva.net/Conferences/2024/SubmissionPolicies">policy</a>
                                        regarding dual submissions. <br/>
                                        Make sure your leaderboard entry dates between the competition start and end
                                        dates (21<sup>st</sup> May, 2024 - 23<sup>rd</sup> August, 2024). <br />
                                        Your leaderboard entry should refer to a valid method description,
                                        <em>e.g.</em>, the title of the <span class="extended-abstract">extended abstract</span>
                                        or the link to your <span class="method-description">method description</span> on ArXiv.
                                    </dd>
                                </div>
                            </dl>
                        </div> <!-- accordion-body -->
                    </div> <!-- accordion-collapse -->
                </div> <!-- accordion-item -->

                <!-- --------------- -->

                <div class="accordion-item">
                    <h2 class="accordion-header" id="headingFour">
                        <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                                data-bs-target="#collapseFour" aria-expanded="false" aria-controls="collapseFour">
                            <span class="accordion-button-emoji-container">üèÜ‚ùå<span class="mx-2"></span>üßæüèóÔ∏è</span>
                        </button>
                    </h2>
                    <div id="collapseFour" class="accordion-collapse collapse" aria-labelledby="headingFour">
                        <div class="accordion-body">
                            <dl>
                                <div class="row">
                                    <dt class="col-auto" style="width: 20px;"><b>Q</b>:</dt>
                                    <dd class="col">
                                        I don't want to participate in the challenge, but I have a paper that has not
                                        been published anywhere else and is not currently under review anywhere. <br />
                                        Can I submit it to the workshop?
                                    </dd>
                                </div> <!-- row Q -->
                                <div class="row">
                                    <dt class="col-auto" style="width: 20px;"><b>A</b>:</dt>
                                    <dd class="col">Yes! Submit it as a <span class="workshop-paper">workshop paper</span>
                                        on the workshop
                                        <a href="https://cmt3.research.microsoft.com/MAPFREEWorkshop2024/Submission/Index">
                                            CMT</a>. <br/>
                                        We encourage you to update the paper with evaluation on the
                                        <a href="https://research.nianticlabs.com/mapfree-reloc-benchmark/dataset">
                                            Niantic Map-free Relocalization Dataset</a>, but it is not required. <br />
                                        If your method can solve the task in the challenge, you should consider also
                                        submitting it to the leaderboard.
                                    </dd>
                                </div> <!-- row A -->
                            </dl>
                        </div> <!-- accordion-body -->
                    </div> <!-- accordion-collapse -->
                </div> <!-- accordion-item -->
            </div> <!-- accordion -->
        </div> <!-- col -->
    </div> <!-- row FAQ -->

    <div class="row mt-4">
        <div class="col-12">
            <p>We look forward to your contributions advancing the field of map-free visual
                relocalization.<br /> For any questions or clarifications, please contact the workshop
                organizers.</p>
        </div>
    </div>

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Organizers -->
    <div class="row mt-4 align-items-center">
        <div class="col-12">
            <h3><a id="Organizers">Organizers</a><!--
            --><small><!--
              --><a
                    onclick="copyLinkToClipboard('Organizers');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h3>
            <hr />
        </div> <!-- col -->
    </div>

    <div class="row justify-content-center">
        <div class="col mugshot-item">
            <a href="https://amonszpart.github.io/">
                <img class="headshot" src="assets/organizers/aron.jpeg"/>
                <span class="name">√Åron Monszpart</span>
                <span class="affiliation">Niantic</span>
            </a>
        </div> <!-- col -->
        <div class="col mugshot-item">
            <a href="https://ebrach.github.io/">
                <div class="item">
                    <img class="headshot" src="assets/organizers/eric.jpeg"/>
                    <span class="name">Eric Brachmann</span>
                    <span class="affiliation">Niantic</span>
                </div><!--
         --></a>
        </div> <!-- col -->
        <div class="col mugshot-item">
            <a href="https://scholar.google.com/citations?user=bSxMRawAAAAJ">
                <div class="item">
                    <img class="headshot" src="assets/organizers/filipe.jpeg"/>
                    <span class="name">Filipe Gaspar</span>
                    <span class="affiliation">Niantic</span>
                </div><!--
         --></a>
        </div> <!-- col -->
        <div class="col mugshot-item">
            <a href="https://guiggh.github.io/">
                <div class="item">
                    <img class="headshot" src="assets/organizers/guillermo.jpeg"/>
                    <span class="name">Guillermo Garcia-Hernando</span>
                    <span class="affiliation">Niantic</span>
                </div><!--
         --></a>
        </div> <!-- col -->
        <div class="col mugshot-item">
            <a href="https://scholar.google.com/citations?user=m_SPRGUAAAAJ&hl=en">
                <div class="item">
                    <img class="headshot" src="assets/organizers/axel.jpeg"/>
                    <span class="name">Axel Barroso-Laguna</span>
                    <span class="affiliation">Niantic</span>
                </div><!--
         --></a>
        </div> <!-- col -->
        <div class="col mugshot-item">
            <a href="https://dantkz.github.io/about/">
                <div class="item">
                    <img class="headshot" src="assets/organizers/daniyar.jpeg"/>
                    <span class="name">Daniyar Turmukhambetov</span>
                    <span class="affiliation">Niantic</span>
                </div><!--
         --></a>
        </div> <!-- col -->
        <div class="col mugshot-item">
            <a href="http://web4.cs.ucl.ac.uk/staff/g.brostow/">
                <div class="item">
                    <img class="headshot" src="assets/organizers/gabriel.jpeg"/>
                    <span class="name">Gabriel J. Brostow</span>
                    <span class="affiliation">Niantic, University College London</span>
                </div><!--
         --></a>
        </div> <!-- col -->
        <div class="col mugshot-item">
            <a href="https://www.robots.ox.ac.uk/~victor/">
                <div class="item">
                    <img class="headshot" src="assets/organizers/victor.jpg"/>
                    <span class="name">Victor Adrian Prisacariu</span>
                    <span class="affiliation">Niantic, University of Oxford</span>
                </div><!--
         --></a>
        </div> <!-- col -->
    </div> <!-- row -->

    <!-- --------------------------------------------------------------------------------------- -->

    <!-- Sponsors -->
    <div class="row justify-content-center">
        <div class="col-12">
            <h3><a id="Sponsors">Sponsors</a><!--
            --><small><!--
              --><a
                    onclick="copyLinkToClipboard('Sponsors');"
                    style="cursor: pointer;"> üîó<!--
              --></a><!--
            --></small><!--
         --></h3>
            <hr />
            <div class="row justify-content-center">
                <div class="col col-sm-2"></div>
                <div class="col-sm-auto d-flex justify-content-center">
                    <a href="https://europe.naverlabs.com">
                        <img alt="Naver Labs Europe" src="assets/images/NLE_3_WHITE.png"
                             style="height: 100px; filter: invert(100)"/>
                    </a>
                </div>
                <div class="col-sm-auto d-flex justify-content-center">
                    <a href="https://nianticlabs.com">
                        <img alt="Niantic" src="assets/images/NianticLogo-Large.png"
                             style="height: 100px;"/>
                    </a>
                </div>
                <div class="col col-sm-2"></div>
            </div>
        </div> <!-- col -->
    </div> <!-- row -->

</div> <!-- container -->

<!-- --------------------------------------------------------------------------------------- -->

<footer style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
    <section>
        <p style="text-align: center;">&copy; 2024 Map-free Challenge organizers</p>
        <p style="text-align: center;">
            <a href="https://github.com/nianticlabs/map-free-reloc">GitHub</a>
            <a href="mailto:map-free-workshop@nianticlabs.com" target="_blank">E-mail</a>
        </p>
    </section>
</footer>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz"
        crossorigin="anonymous"></script>

<script src="https://code.jquery.com/jquery-3.7.1.min.js"
        integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<script src="js/scripts_niantic.js" type="text/javascript"></script>
<script type="text/javascript">
    document.addEventListener('DOMContentLoaded', function() {
        setSubmissionTypePill('.workshop-paper', 'bg-success', '#Workshop_paper');
        setSubmissionTypePill('.extended-abstract', 'bg-warning', '#Extended_abstract');
        setSubmissionTypePill('.method-description', 'bg-info', '#Method_description');
    });


    document.addEventListener('DOMContentLoaded', function() {
        addCountdown("day-start-countdown", "2024/05/21 00:00:00 UTC");
        addCountdown("day-paper-submit-countdown", "2024/08/02 23:59:59 UTC-12");
        addCountdown("day-paper-camera-ready-countdown", "2024/08/22 23:59:59 UTC-12");
        addCountdown("day-end-countdown", "2024/08/23 23:59:59 UTC-12");
        addCountdown("day-describe-countdown", "2024/08/27 23:59:59 UTC-12");
        addCountdown("day-winners-countdown", "2024/08/30 23:59:59 UTC-12");
        addCountdown("day-workshop-countdown", "2024/09/30 08:00:00 UTC+2");
    });

    // Hide menu on menu item click
    document.addEventListener('DOMContentLoaded', function() {
        // Navbar click collapses menu, see https://stackoverflow.com/a/42401686
        const menuToggle = document.getElementById('navbarSupportedContent')
        const bsCollapse = bootstrap.Collapse.getOrCreateInstance(menuToggle, {toggle: false})

        // Add event listener to all nav items and dropdown-items
        for (navClass of ['.nav-item', '.dropdown-item']) {
            const navLinks = document.querySelectorAll(navClass);
            navLinks.forEach((l) => {
                if (!l.classList.contains('dropdown')) {
                    l.addEventListener('click', () => {
                        bsCollapse.toggle();
                    }); // addEventListener('click')
                } // if not dropdown
            }); // forEach navLinks
        } // for navClass
    }); // addEventListener('DOMContentLoaded')

    document.addEventListener('DOMContentLoaded', function() {
        setPrizeLocales();
    }); // addEventListener('DOMContentLoaded')

</script>

</body>
</html>
