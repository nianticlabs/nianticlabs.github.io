<!DOCTYPE html>
<html lang="en">

<head>
  <title>DoubleTake</title>
  <meta name="description" content="DoubleTake: Geometry Guided Depth Estimation, 2024">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!--Facebook preview-->
  <meta property="og:image" content="https://nianticlabs.github.io/doubletake/resources/social_card.png">
  <!--   <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="600">
  <meta property="og:image:height" content="400"> -->
  <meta property="og:type" content="website" />
  <meta property="og:url" content="..." />
  <meta property="og:title" content="DoubleTake: Geometry Guided Depth Estimation" />
  <meta property="og:description" content="DoubleTake: Geometry Guided Depth Estimation, 2024" />

  <!--Twitter preview-->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="DoubleTake: Geometry Guided Depth Estimation" />
  <meta name="twitter:description" content="DoubleTake: Geometry Guided Depth Estimation, 2024" />
  <meta name="twitter:image" content="https://nianticlabs.github.io/doubletake/resources/social_card.png">

  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> -->
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
  <script src="https://kit.fontawesome.com/746ee6bfa4.js" crossorigin="anonymous"></script>

  <link rel="icon" href="resources/favi_2.png">

</head>

<body>

  <div class="container" style="text-align:center; padding:2rem 15px">
    <div class="row" style="text-align:center">
      <h1 style="margin-bottom:0.1rem;">DoubleTake</h1>
      <h3 style="margin-top:0.1rem;margin-bottom:0.5rem;">Geometry Guided Depth Estimation</h3>
    </div>
    <div class="row" style="text-align:center">
      <div class="col-xs-0 col-md-2"></div>
      <div class="col-xs-12 col-md-8">
        <h4>
          <a style="font-size:1.1em;" href="https://masayed.com/">
            <nobr>Mohamed Sayed<sup>1</sup></nobr>
          </a> &emsp;
          <a style="font-size:1.1em;" href="https://filippoaleotti.github.io/website/">
            <nobr>Filippo Aleotti<sup>1</sup></nobr>
          </a> &emsp;
          <a style="font-size:1.1em;" href="https://www.linkedin.com/in/jamie-watson-544825127/">
            <nobr>Jamie Watson<sup>1, 2</sup></nobr>
          </a> &emsp;
          <a style="font-size:1.1em;" href="https://qureshizawar.github.io/">
            <nobr>Zawar Qureshi<sup>1</sup></nobr>
          </a> &emsp;
          <a style="font-size:1.1em;" href="https://guiggh.github.io/">
            <nobr>Guillermo Garcia-Hernando<sup>1</sup></nobr>
          </a> &emsp;
          <a style="font-size:1.1em;" href="http://www0.cs.ucl.ac.uk/staff/g.brostow/">
            <nobr>Gabriel Brostow<sup>1,2</sup></nobr>
          </a> &emsp;
          <a style="font-size:1.1em;" href="https://scholar.google.co.uk/citations?user=7wWsNNcAAAAJ&hl=en">
            <nobr>Sara Vicente<sup>1</sup></nobr>
          </a> &emsp;
          <a style="font-size:1.1em;" href="http://www.michaelfirman.co.uk/">
            <nobr>Michael Firman<sup>1</sup></nobr>
          </a> &emsp;
        </h4>

        <sup>1</sup>Niantic&nbsp;&nbsp;&nbsp;<sup>2</sup>University College London&nbsp;&nbsp;&nbsp;

      </div>

      <div class="col-xs-0 col-md-2"></div>

    </div>

  </div>


  <div class="container" style="max-width: 1100px">
    <div class="row" style="text-align: center; padding:1rem">
      <div class="col-xs-3"></div>
      <div class="col-xs-2">
        <a href="resources/DoubleTake.pdf" style="color:inherit">
          <a href="resources/DoubleTake.pdf" style="color:#ff730">
            <i class="fa-solid fa-file-pdf fa-4x"></i></a>
      </div>
      <div class="col-xs-2">
        <a href="https://github.com/nianticlabs/DoubleTake" style="color:inherit;">
          <a href="https://github.com/nianticlabs/DoubleTake" style="color:#ff730">
            <i class="fa fa-github fa-4x"></i></a><br>
            Coming soon!
      </div>

      <div class="col-xs-2">
        <a href="https://youtu.be/IklQ5AHNdI8" style="color:inherit;">
          <a href="https://youtu.be/IklQ5AHNdI8" style="color:">
            <i class="fa-brands fa-youtube fa-4x"></i></a>
      </div>

      <div class="col-xs-3"></div>

    </div>
  </div>

  <div class="container" style="text-align:center; padding:1rem">
    <br>
    <img src="resources/teaser.png" alt="teaser.png" class="text-center" style="width: 100%; max-width: 1100px">
  </div>

  <div class="container">
    <h3>Abstract</h3>
    <hr />
    <p>
      Estimating depth from a sequence of posed RGB images is a fundamental computer vision task, with applications in
      augmented reality, path planning etc.
      Prior work typically makes use of previous frames in a multi view stereo framework,
      relying on matching textures in a local neighborhood.
      In contrast, our model leverages historical predictions by giving the latest 3D geometry data as an extra input to
      our network.
      This <b>self-generated geometric hint</b> can encode information from areas of the scene not covered by the keyframes and
      it is more regularized when compared to individual predicted depth maps for previous frames.
      We introduce a <i>Hint MLP</i> which combines cost volume features with a hint of the prior geometry, rendered as
      a depth map from the current camera location, together with a measure of the confidence in the prior geometry.
      We demonstrate that our method, which can run at interactive speeds, achieves <b>state-of-the-art estimates of depth
      and 3D scene reconstruction</b> in <b><span style="color: rgb(255, 0, 96)">offline</span></b>, <b><span style="color: rgb(0, 190, 62)">incremental</span></b>, and <b><span style="color: rgb(0, 121, 255)">revisit</span></b> evaluation scenarios.
    </p>

    <br>
    <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <!-- <h4><u>Real Scenes</u></h4> -->
      <div class="col-xs-12">
        <div class="video-container">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/IklQ5AHNdI8" title="YouTube video player"
            frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
            allowfullscreen></iframe>
        </div>
      </div>
    </div>

    <br>
    <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <div class="col-xs-12">
        <video controls autoplay loop muted width="95%">
          <source src="resources/OODResults_2_1.mp4" type="video/mp4">
        </video>
      </div>
    </div> 

    <div class="row" style="text-align: center">
      <div class="col-xs-12">
        <div>
          Our ScanNetV2 trained-model can also get high quality <span style="color: rgb(255, 0, 96)">offline</span> reconstructions via TSDF fusion in just 13.8s on average.
        </div>
      </div>
    </div>


    <!-- <br>
    <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">

      <div class="col-xs-12">
        <video loop controls width="95%">
          <source src="resources/birdseye_recon.mp4" type="video/mp4">
        </video>
      </div>
    </div>
    <br>
    <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">

      <div class="col-xs-12">
        <video loop controls width="95%">
          <source src="resources/living_room_birdseye.mp4" type="video/mp4">
        </video>
      </div>
    </div> -->

    <h3>Approach</h3>
    <hr />

    <div class="row" style="text-align: center">
      <div class="col-xs-12">
        <h4 style="font-size:1.6em;"><u>Overview (<span style="color: rgb(0, 190, 62)">incremental</span>)</h4></u></h4>
      </div>
    </div>


    <!-- <div class="row" style="text-align: center">
      <div class="col-xs-12">
        <img src="resources/system_overview.png" alt="system_overview.png" class="text-center" style="width: 90%; max-width: 900px; margin-top:
      10px"><br /><br /><br />
      </div>
    </div> -->

    <br>
    <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <div class="col-xs-12">
        <video autoplay loop muted width="95%">
          <source src="resources/IncrmentalViz_extended_1.mp4" type="video/mp4">
        </video>
      </div>
    </div> 
    
    <div class="row" style="text-align: center">
      <div class="col-xs-12">
        <div>
          <p>
            Our key contribution is the injection of cheaply-available <b>metadata</b>
            into the feature volume. Each volumetric cell is then reduced in parallel with an MLP into a feature map
            before
            input into a 2D cost volume encoder-decoder. We also make use of an image encoder specifically used to
            enforce a strong image
            prior when propagating and correcting depth estimates from the cost volume throughout the frame in the cost
            volume encoder-decoder. This formulation is flexible and allows for three different operating modes: 
            1) <span style="color: rgb(0, 190, 62)">incremental</span> for online depth and reconstruction at <b>76.6ms per frame</b>, 
            2) <span style="color: rgb(255, 0, 96)">offline</span> for high-quality offline depth and reconstruction at <b>13.8s per scene</b>, 
            and 3) <span style="color: rgb(0, 121, 255)">revisit</span> for depth estimation when revisiting locations previously seen at <b>62.8ms per frame</b>.   

        </div>
      </div>
    </div>

    <div class="row" style="text-align: center">
      <div class="col-xs-12">
        <h4 style="font-size:1.6em;"><u>Hint MLP - Geometry Injection</u></h4>
      </div>
    </div>


    <div class="row" style="text-align: center">
      <div class="col-xs-12">
        <img src="resources/method_detail.png" alt="method_detail.png" class="text-center" style="width: 90%; max-width: 900px; margin-top:
      10px"> <br /><br /><br />
      </div>
    </div>

    <div class="row" style="text-align: center">
      <div class="col-xs-12">
        <div>
          <p><b>Geometry Injection</b> Our feature volume is reduced to a cost volume via a <i>matching MLP</i>.  
            Our <i>Hint MLP</i> then combines the multi-view-stereo cost volume with an estimate of previously predicted geometry.  
            For every location in the cost volume, the <i>Hint MLP</i> takes as input (i) the visual matching score, (ii) the geometry hint, 
            formed as the absolute difference between the rendered depth hint and the depth plane at that cost volume position, and (iii) an 
            estimate of the confidence of the hint at that pixel. 
        </div>
      </div>
    </div>


    <h3>Results</h3>
    <hr />





    <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <h4 style="font-size:1.6em;">Online Depths - <span style="color: rgb(0, 190, 62)">incremental</span></h4>
      <div class="col-xs-12">
        <img src="resources/Ours_incremental_depth.png" alt="Ours_incremental_depth.png" class="text-center" style="width: 90%; max-width: 900px; margin-top:
      10px">
      </div>
    </div>
    <br>

    <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <h4 style="font-size:1.6em;">Online Reconstructions - <span style="color: rgb(0, 190, 62)">incremental</span></h4>
      <div class="col-xs-12">
        <img src="resources/incremental_recon.png" alt="Ours_incremental_depth.png" class="text-center" style="width: 90%; max-width: 900px; margin-top:
      10px">
      </div>
    </div>
    <br>

    <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <h4 style="font-size:1.6em;">Offline Depths - <span style="color: rgb(255, 0, 96)">offline</span></h4>
      <div class="col-xs-12">
        <img src="resources/offline_depth.png" alt="offline_depth.png" class="text-center" style="width: 90%; max-width: 900px; margin-top:
      10px">
      </div>
    </div>
    <br>


    <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <h4 style="font-size:1.6em;">Offline Reconstructions - <span style="color: rgb(255, 0, 96)">offline</span></h4>
      <div class="col-xs-12">
        <img src="resources/offline_recon.png" alt="offline_recon.png" class="text-center" style="width: 90%; max-width: 900px; margin-top:
      10px">
      </div>
    </div>
    <br>

    <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <h4 style="font-size:1.6em;">Depths for Revisiting a Location - <span style="color: rgb(0, 121, 255)">revisit</span></h4>
      <div class="col-xs-12">
        <img src="resources/revisit.png" alt="revisit.png" class="text-center" style="width: 90%; max-width: 900px; margin-top:
      10px">
      </div>
    </div>
    <br>

    <h3>Resources</h3>
    <hr />
    <div class="row" style="text-align: center">
      <div class="col-xs-0 col-lg-0"></div>
      <div class="col-xs-2 col-lg-2"></div>
      <div class="col-xs-4 col-lg-4">
        <h4>Paper</h4>
        <a href="resources/DoubleTake.pdf" style="color:#ff730">
          <img src="resources/paper_thumb.png" alt="Paper" class="text-center" style="max-width:70%; border:0.15em solid;
        border-radius:0.5em;"></a>
      </div>
      
      <div class="col-xs-4 col-lg-4">
        <h4>Supplemental</h4>
        <a href="resources/DoubleTakeSupplemental.pdf" style="color:#ff730;">
          <img src="resources/supp_thumb.png" alt="Supplemental" class="text-center"
          style="max-width:70%; border:0.15em solid;border-radius:0.5em;"></a>
      </div>

      <div class="col-xs-2 col-lg-2"></div>
      <!-- <div class="col-xs-4 col-lg-4">
        <h4>Code</h4>
        <a href="https://github.com/nianticlabs/DoubleTake" style="color:inherit;">
          <img src="resources/github_repo.png" alt="github_repo.png" class="text-center"
            style="max-width:70%; border:0em solid;border-radius:0.5em;"></a>
      </div> -->

      <div class="col-xs-0 col-lg-0"></div>
    </div>
    <h4 style="padding-top:0.5em">BibTeX</h4>
    <p>If you find this work useful for your research, please cite:</p>
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
    @inproceedings{sayed2024DoubleTake,
      title={DoubleTake: Geometry Guided Depth Estimation}, 
      author={Mohamed Sayed and Filippo Aleotti and Jamie Watson and Zawar Qureshi and Guillermo Garcia-Hernando and Gabriel Brostow and Sara Vicente and Michael Firman},
      year={2024},
      eprint={2406.18387},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.18387}, 
    }
        </pre>
      </div>
    </div>

<!-- 

    <h3>Acknowledgements</h3>
    <hr />
    <p>We thank Aljaž Božič of <a href="https://aljazbozic.github.io/transformerfusion/">TransformerFusion</a>, Jiaming
      Sun of <a href="https://zju3dv.github.io/neuralrecon/">Neural Recon</a>,
      and Arda Düzçeker of <a href="https://github.com/ardaduz/deep-video-mvs">DeepVideoMVS</a> for quickly providing
      useful information to help with baselines and for making their codebases readily available, especially on short
      notice.</p>

    <p>The tuple generation scripts make heavy use of a modified version of DeepVideoMVS's Keyframe buffer (thanks again
      Arda and co!).</p>

    <p>The PyTorch point cloud fusion module is borrowed from <a
        href="https://github.com/alexrich021/3dvnet">3DVNet's</a> repo. Thanks Alexander Rich!</p>

    <p>We'd also like to thank Niantic's infrastructure team for quick actions when we needed them. Thanks folks!</p>

    <p>Mohamed is funded by a Microsoft Research PhD Scholarship (MRL 2018-085).</p> -->
  </div>

  <div class="container" style="padding-top:3rem; padding-bottom:3rem">
    <p style="text-align:center">
      &#169; This webpage was in part inspired from this
      <a href="https://github.com/monniert/project-webpage">template</a>.
    </p>
  </div>

</body>

</html>